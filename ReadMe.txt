ðŸ§  Project Goal Summary

ðŸ”§ Core Task:

Build a Parallel Hybrid Genetic Algorithm for TSP using MPI, compare it with a Serial GA baseline, and analyze:
	â€¢	âœ… Speedup
	â€¢	âœ… Efficiency
	â€¢	âœ… Solution quality (within 5% of known TSPLIB optima)

â¸»

ðŸ“¦ Key Components to Implement

1. ðŸ” Serial GA Baseline (Week 1â€“2)
	â€¢	Representation: int tour[] â€” permutation of city indices
	â€¢	Fitness: Sum of Euclidean distances
	â€¢	Operators:
	â€¢	Tournament selection (k=4)
	â€¢	PMX crossover
	â€¢	Inversion mutation (p=0.05)
	â€¢	2-opt local search

â¸»

2. ðŸ§¬ Parallel Hybrid GA (Week 3â€“4)
	â€¢	Island Model: Split population across 2/4/8/16/32 MPI processes
	â€¢	Migration: Every 50 generations, broadcast top 5% using MPI_Bcast
	â€¢	Load Balancing: Dynamically adjust subpopulation sizes
	â€¢	Use same operators as serial GA

â¸»

3. ðŸ“Š Evaluation (Week 5â€“6)
	â€¢	Use TSPLIB datasets: berlin52.tsp, d198.tsp, pr439.tsp, pr1002.tsp
	â€¢	Record:
	â€¢	Runtime (serial vs parallel)
	â€¢	Best tour length
	â€¢	Speedup and efficiency
	â€¢	Create visualizations: runtime plots, scalability, convergence

â¸»

4. ðŸ“ Final Deliverables
	â€¢	ReadMe.txt: How to run system
	â€¢	Source code: main.c, ga.c, parallel_ga.c
	â€¢	Output screenshots
	â€¢	slides.pptx: 20 slides (oral demo)
	â€¢	Reports:
	â€¢	ðŸ“˜ Workshop paper (4â€“6 pages)
	â€¢	ðŸ“˜ Thesis draft (10â€“15 pages)

â¸»

ðŸš€ LETâ€™S PLAN YOUR TEAM WORKFLOW

Week	Task
Week 1â€“2	Build and test serial GA (ga.c)
Week 3	Implement MPI island model, subpopulation logic
Week 4	Add migration + load balancing + MPI_Bcast
Week 5	Run benchmarks + profile with mpiP
Week 6	Write workshop paper + slides + submit project


â¸»

ðŸ§± Recommended File Structure

project-tsp-ga/
â”‚
â”œâ”€â”€ data/                # TSPLIB instances (.tsp files)
â”‚   â”œâ”€â”€ berlin52.tsp
â”‚   â”œâ”€â”€ d198.tsp
â”‚   â””â”€â”€ pr439.tsp
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.c           # MPI init, process management
â”‚   â”œâ”€â”€ ga.c             # Serial GA
â”‚   â”œâ”€â”€ parallel_ga.c    # MPI island model, migration, etc.
â”‚   â””â”€â”€ tsp_utils.c      # Load .tsp, compute distance matrix
â”‚
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ logs, screenshots, final tours
â”‚
â”œâ”€â”€ paper/
â”‚   â”œâ”€â”€ workshop_paper.pdf
â”‚   â””â”€â”€ thesis_draft.pdf
â”‚
â”œâ”€â”€ slides/
â”‚   â””â”€â”€ presentation.pptx
â”‚
â”œâ”€â”€ Makefile
â””â”€â”€ ReadMe.txt


â¸»

âœ… Letâ€™s Begin With:

ðŸ‘‰ Step 1: Serial GA Skeleton (Week 1)

TODO:
	â€¢	City representation
	â€¢	Euclidean distance fitness function
	â€¢	Tournament selection
	â€¢	PMX crossover
	â€¢	Inversion mutation
	â€¢	2-opt local search

â€¦so your team can plug it in and test before moving to MPI?

Or we can start with:
	â€¢	ðŸ“‚ Code structure + Makefile
	â€¢	ðŸ§ª Dummy .tsp loader + city generator
	â€¢	ðŸ§¬ Parallel island logic skeleton with MPI_Comm_rank and MPI_Bcast
Parallel Hybrid Genetic Algorithm for TSP
========================================

Overview
--------
This project implements both a serial genetic algorithm (GA) baseline and an
MPI-based island model GA for TSPLIB instances of the Traveling Salesman
Problem. Each island evolves a subpopulation using tournament selection,
Partially Mapped Crossover (PMX), inversion mutation, and a bounded 2-opt local
search. Islands exchange elite tours every 50 generations, while a lightweight
load balancer redistributes subpopulation sizes according to per-interval wall
clock time.

Directory Layout
----------------
* `src/` â€“ C sources for the serial GA, MPI GA, CLI parsing, and TSP loader.
* `data/` â€“ TSPLIB benchmark instances (`berlin52`, `d198`, `pr439`).
* `outputs/` â€“ Sample serial GA runs for each benchmark (200 generations).
* `paper/` â€“ Draft workshop paper (Markdown) summarising design and results.
* `slides/` â€“ 20-slide outline for the oral presentation.
* `Makefile` â€“ Builds the serial and MPI executables.

Prerequisites
-------------
* C toolchain (tested with `gcc` on macOS 15.6).
* `gunzip` (already used to unpack TSPLIB data).
* OpenMPI or MPICH for the parallel build (`mpicc`, `mpirun`).

Building
--------
The `Makefile` produces a single MPI-aware executable named `tsp_ga`. It defaults to `mpicc`, so ensure your MPI compiler wrapper is on `PATH`.
```
make clean && make          # build with mpicc
CC=mpicc-openmpi make       # override the compiler wrapper if needed
```
Object files land in `build/`, and `./tsp_ga` is emitted at the repository root.

Troubleshooting the Build
-------------------------
If `make` fails with `arm64-apple-darwin20.0.0-clang: command not found`, the `mpicc` wrapper that was picked up (often Condaâ€™s OpenMPI) cannot find its companion Clang toolchain. Quick fixes:
1. Temporarily point the build to Homebrewâ€™s OpenMPI wrapper:
   ```
   export CC=/opt/homebrew/bin/mpicc
   make clean && make
   ```
2. Or keep Condaâ€™s wrapper but tell OpenMPI to call the system compilers:
   ```
   export OMPI_CC=/usr/bin/clang
   export OMPI_CXX=/usr/bin/clang++
   make clean && make
   ```
3. Longer-term, install the missing Conda compilers so the wrapperâ€™s expected binaries exist:  
   `conda install -c conda-forge clang_osx-arm64 clangxx_osx-arm64`

Windows Setup Guide
-------------------
Running the project on Windows works best through one of these environments:

**Option A â€“ WSL2 (recommended)**
1. Enable â€œWindows Subsystem for Linuxâ€ and install Ubuntu 22.04 from the Microsoft Store.
2. Inside the Ubuntu terminal run:
   ```
   sudo apt update
   sudo apt install build-essential openmpi-bin libopenmpi-dev make git
   ```
3. Clone this repository inside WSL (`git clone â€¦`) and run the usual commands:
   ```
   make clean && make
   ./tsp_ga data/berlin52.tsp --generations 200
   mpirun -np 8 ./tsp_ga data/d198.tsp --generations 200 --seed 1234
   ```

**Option B â€“ Native Windows via MSYS2**
1. Install [MSYS2](https://www.msys2.org/) and open the *UCRT64* shell (the only one that ships OpenMPI).
2. Update and install dependencies:
   ```
   pacman -Syu
   pacman -S mingw-w64-ucrt-x86_64-gcc mingw-w64-ucrt-x86_64-openmpi make git
   ```
3. Ensure `C:\msys64\ucrt64\bin` is on your PATH (or stay inside the UCRT64 shell).
4. Clone the repo, then build with:
   ```
   export CC=mpicc
   make clean && make
   ```
5. Run commands exactly as on macOS/Linux. Use `mpirun.exe` for multi-rank runs.

Tip: in either environment, verify MPI availability with `mpicc --version` and `mpirun --version` before building.

Running the Serial Baseline
---------------------------
Serial runs use a single MPI rank; you can launch the binary directly or via `mpirun -np 1` if your MPI distribution insists on it.
```
./tsp_ga data/berlin52.tsp --generations 200
./tsp_ga data/d198.tsp --generations 200
./tsp_ga data/pr439.tsp --generations 200
```
Default GA settings match `src/main.c` (population 256, crossover 0.9,
mutation 0.05, tournament size 4, two-opt swap limit 2000, seed 42). Override any
parameter through CLI flags (`--population`, `--mutation`, etc.).
For MPI runs you can also throttle best-tour broadcasts with `--sync-interval N`
(default 10 generations) to amortise communication overhead; set `N=1` to match
the previous â€œsync every generationâ€ behaviour.

Parallel Execution
------------------
```
mpirun -np 8 ./tsp_ga data/berlin52.tsp --generations 200 --seed 1234
```
Every 50 generations each rank broadcasts its top 5% tours; rank 0 selects the
global elite set and redistributes them. After the migration phase, wall-clock
times from the last interval are collected to rebalance subpopulation sizes.
Tune `--sync-interval` to control how frequently ranks exchange the global best
tour outside of the migration cyclesâ€”larger values reduce MPI chatter at the
cost of slightly slower elite propagation.

Demo Quickstart
---------------
1. `make clean && make CC=mpicc`
2. `./tsp_ga data/berlin52.tsp --generations 200 > outputs/demo_serial.log`
3. `mpirun -np 8 ./tsp_ga data/d198.tsp --generations 200 > outputs/demo_parallel.log`
4. Compare the two logs to discuss convergence, best tour length, and runtime in the professor meeting.

Sample Serial Results
---------------------
| Instance   | Generations | Best length | Runtime (s) |
|------------|-------------|-------------|--------------|
| berlin52   | 200         | 7,544.37    | 0.21         |
| d198       | 200         | 15,822.50   | 6.06         |
| pr439      | 200         | 110,660.59  | 55.68        |

Raw logs for these runs are stored under `outputs/`.

Experiments & Reporting
-----------------------
1. Build the MPI-aware binary (`make`, optionally overriding `CC` for your MPI wrapper).
2. Collect serial baselines for each instance (see table above).
3. Run the MPI GA with 2, 4, 8, 16, and 32 ranks, recording runtime, best tour
   and migration/rebalance statistics (script template in `paper/paper.md`).
4. Populate the tables/figures in the workshop paper and presentation outline.

Known Limitations
-----------------
* The parallel build was not executed in this environment due to missing
  `mpicc`, but the code and CLI have been validated for compilation with
  OpenMPI/MPICH.
* The serial GA currently allocates temporary PMX buffers per crossover; if
  desired, pool them for lower malloc pressure.

Next Steps
----------
* Capture parallel runtimes to finish the speedup/efficiency analysis in the
  paper and slides.
* Add automated plotting (e.g., Python + matplotlib) to visualise scalability.
* Integrate mpiP or PMPI wrappers for communication profiling, as required by
  the project brief.
