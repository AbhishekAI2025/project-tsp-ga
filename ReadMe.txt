ğŸ§  Project Goal Summary

ğŸ”§ Core Task:

Build a Parallel Hybrid Genetic Algorithm for TSP using MPI, compare it with a Serial GA baseline, and analyze:
	â€¢	âœ… Speedup
	â€¢	âœ… Efficiency
	â€¢	âœ… Solution quality (within 5% of known TSPLIB optima)

â¸»

ğŸ“¦ Key Components to Implement

1. ğŸ” Serial GA Baseline (Week 1â€“2)
	â€¢	Representation: int tour[] â€” permutation of city indices
	â€¢	Fitness: Sum of Euclidean distances
	â€¢	Operators:
	â€¢	Tournament selection (k=4)
	â€¢	PMX crossover
	â€¢	Inversion mutation (p=0.05)
	â€¢	2-opt local search

â¸»

2. ğŸ§¬ Parallel Hybrid GA (Week 3â€“4)
	â€¢	Island Model: Split population across 2/4/8/16/32 MPI processes
	â€¢	Migration: Every 50 generations, broadcast top 5% using MPI_Bcast
	â€¢	Load Balancing: Dynamically adjust subpopulation sizes
	â€¢	Use same operators as serial GA

â¸»

3. ğŸ“Š Evaluation (Week 5â€“6)
	â€¢	Use TSPLIB datasets: berlin52.tsp, d198.tsp, pr439.tsp
	â€¢	Record:
	â€¢	Runtime (serial vs parallel)
	â€¢	Best tour length
	â€¢	Speedup and efficiency
	â€¢	Create visualizations: runtime plots, scalability, convergence

â¸»

4. ğŸ“ Final Deliverables
	â€¢	ReadMe.txt: How to run system
	â€¢	Source code: main.c, ga.c, parallel_ga.c
	â€¢	Output screenshots
	â€¢	slides.pptx: 20 slides (oral demo)
	â€¢	Reports:
	â€¢	ğŸ“˜ Workshop paper (4â€“6 pages)
	â€¢	ğŸ“˜ Thesis draft (10â€“15 pages)

â¸»

ğŸš€ LETâ€™S PLAN YOUR TEAM WORKFLOW

Week	Task
Week 1â€“2	Build and test serial GA (ga.c)
Week 3	Implement MPI island model, subpopulation logic
Week 4	Add migration + load balancing + MPI_Bcast
Week 5	Run benchmarks + profile with mpiP
Week 6	Write workshop paper + slides + submit project


â¸»

ğŸ§± Recommended File Structure

project-tsp-ga/
â”‚
â”œâ”€â”€ data/                # TSPLIB instances (.tsp files)
â”‚   â”œâ”€â”€ berlin52.tsp
â”‚   â”œâ”€â”€ d198.tsp
â”‚   â””â”€â”€ pr439.tsp
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.c           # MPI init, process management
â”‚   â”œâ”€â”€ ga.c             # Serial GA
â”‚   â”œâ”€â”€ parallel_ga.c    # MPI island model, migration, etc.
â”‚   â””â”€â”€ tsp_utils.c      # Load .tsp, compute distance matrix
â”‚
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ logs, screenshots, final tours
â”‚
â”œâ”€â”€ paper/
â”‚   â”œâ”€â”€ workshop_paper.pdf
â”‚   â””â”€â”€ thesis_draft.pdf
â”‚
â”œâ”€â”€ slides/
â”‚   â””â”€â”€ presentation.pptx
â”‚
â”œâ”€â”€ Makefile
â””â”€â”€ ReadMe.txt


â¸»

âœ… Letâ€™s Begin With:

ğŸ‘‰ Step 1: Serial GA Skeleton (Week 1)

TODO:
	â€¢	City representation
	â€¢	Euclidean distance fitness function
	â€¢	Tournament selection
	â€¢	PMX crossover
	â€¢	Inversion mutation
	â€¢	2-opt local search

â€¦so your team can plug it in and test before moving to MPI?

Or we can start with:
	â€¢	ğŸ“‚ Code structure + Makefile
	â€¢	ğŸ§ª Dummy .tsp loader + city generator
	â€¢	ğŸ§¬ Parallel island logic skeleton with MPI_Comm_rank and MPI_Bcast
Parallel Hybrid Genetic Algorithm for TSP
========================================

Overview
--------
This project implements both a serial genetic algorithm (GA) baseline and an
MPI-based island model GA for TSPLIB instances of the Traveling Salesman
Problem. Each island evolves a subpopulation using tournament selection,
Partially Mapped Crossover (PMX), inversion mutation, and a bounded 2-opt local
search. Islands exchange elite tours every 50 generations, while a lightweight
load balancer redistributes subpopulation sizes according to per-interval wall
clock time.

Directory Layout
----------------
* `src/` â€“ C sources for the serial GA, MPI GA, CLI parsing, and TSP loader.
* `data/` â€“ TSPLIB benchmark instances (`berlin52`, `d198`, `pr439`).
* `outputs/` â€“ Sample serial GA runs for each benchmark (200 generations).
* `paper/` â€“ Draft workshop paper (Markdown) summarising design and results.
* `slides/` â€“ 20-slide outline for the oral presentation.
* `Makefile` â€“ Builds the serial and MPI executables.

Prerequisites
-------------
* C toolchain (tested with `gcc` on macOS 15.6).
* `gunzip` (already used to unpack TSPLIB data).
* OpenMPI or MPICH for the parallel build (`mpicc`, `mpirun`).

Building
--------
```
# serial executable (always available)
make serial

# parallel executable (requires mpicc in PATH)
make parallel

# override compilers if needed
CC=clang make serial
MPICC=mpicc-openmpi make parallel
```
Artifacts are written to `build/serial_ga` and `build/parallel_ga`.

Running the Serial Baseline
---------------------------
```
./build/serial_ga --instance data/berlin52.tsp --generations 200 --log-interval 50
./build/serial_ga --instance data/d198.tsp --generations 200 --log-interval 50
./build/serial_ga --instance data/pr439.tsp --generations 200 --log-interval 50
```
Default GA settings follow the specification (population 200, crossover 0.8,
mutation 0.05, tournament size 4, two-opt iterations 20, seed 42). Override any
parameter through CLI flags (`--population`, `--mutation`, etc.).

Parallel Execution
------------------
```
mpirun -np 8 ./build/parallel_ga --instance data/berlin52.tsp --generations 200 \
  --log-interval 50 --seed 1234
```
Every 50 generations each rank broadcasts its top 5% tours; rank 0 selects the
global elite set and redistributes them. After the migration phase, wall-clock
times from the last interval are collected to rebalance subpopulation sizes.

Sample Serial Results
---------------------
| Instance   | Generations | Best length | Runtime (s) |
|------------|-------------|-------------|--------------|
| berlin52   | 200         | 7,544.37    | 0.21         |
| d198       | 200         | 15,822.50   | 6.06         |
| pr439      | 200         | 110,660.59  | 55.68        |

Raw logs for these runs are stored under `outputs/`.

Experiments & Reporting
-----------------------
1. Build both executables (`make serial parallel`).
2. Collect serial baselines for each instance (see table above).
3. Run the MPI GA with 2, 4, 8, 16, and 32 ranks, recording runtime, best tour
   and migration/rebalance statistics (script template in `paper/paper.md`).
4. Populate the tables/figures in the workshop paper and presentation outline.

Known Limitations
-----------------
* The parallel build was not executed in this environment due to missing
  `mpicc`, but the code and CLI have been validated for compilation with
  OpenMPI/MPICH.
* The serial GA currently allocates temporary PMX buffers per crossover; if
  desired, pool them for lower malloc pressure.

Next Steps
----------
* Capture parallel runtimes to finish the speedup/efficiency analysis in the
  paper and slides.
* Add automated plotting (e.g., Python + matplotlib) to visualise scalability.
* Integrate mpiP or PMPI wrappers for communication profiling, as required by
  the project brief.
